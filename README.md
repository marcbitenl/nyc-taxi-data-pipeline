# üöÄ Pipeline de Dados com Azure Data Factory, Databricks e Delta Lake

## üìå Vis√£o Geral do Projeto

Este projeto demonstra a constru√ß√£o de um pipeline completo de dados para processar informa√ß√µes de corridas de t√°xi da cidade de Nova York. As etapas incluem:

- **Azure Data Factory (ADF)** para extra√ß√£o din√¢mica de dados diretamente de um site p√∫blico.
- **Azure Data Lake Storage (ADLS Gen2)** para armazenamento dos dados em camadas (Bronze, Silver e Gold).
- **Azure Databricks** para transforma√ß√£o e otimiza√ß√£o dos dados utilizando o formato Delta Lake.
- **Unity Catalog** para gerenciar acessos e configurar locais externos de maneira segura.

O pipeline automatiza a coleta de arquivos Parquet, realiza a transforma√ß√£o dos dados no Databricks e armazena os resultados em Delta Lake seguindo uma arquitetura em camadas. Al√©m disso, o projeto demonstra boas pr√°ticas em governan√ßa de dados, controle de acesso e automa√ß√£o de fluxos de trabalho.

---

## ‚öôÔ∏è Tecnologias Utilizadas

- **Azure Data Factory (ADF)**
- **Azure Data Lake Storage Gen2**
- **Azure Databricks** (com Delta Lake e PySpark)
- **Unity Catalog**
- **GitHub** para controle de vers√£o
- **OAuth 2.0** para autentica√ß√£o
- **PySpark** para transforma√ß√£o de dados em larga escala
- **SQL** para consultas e manipula√ß√£o de dados estruturados

---

## üöÄ Como o Projeto Foi Desenvolvido

### üîÑ **1. Extra√ß√£o de Dados com Azure Data Factory (ADF)**

O ADF foi configurado para buscar dados dinamicamente de uma URL p√∫blica, automatizando a extra√ß√£o de arquivos mensais de corridas de t√°xi. A configura√ß√£o incluiu:

1. Cria√ß√£o de um pipeline com um loop **ForEach** para iterar pelos 12 meses do ano.
2. Configura√ß√£o de uma condi√ß√£o (**IfCondition**) para ajustar o formato do m√™s:
   - Para meses de 1 a 9, adicionou-se um zero √† esquerda na URL.
   - Para meses de 10 a 12, o n√∫mero do m√™s foi utilizado diretamente.
3. Configura√ß√£o de uma atividade de **Copy Data** para transferir os arquivos diretamente para o Azure Data Lake (camada Bronze).
4. Automa√ß√£o do agendamento das execu√ß√µes para garantir atualiza√ß√µes peri√≥dicas dos dados.

### üîí **2. Processamento de Dados no Azure Databricks**

Ap√≥s a extra√ß√£o, os dados foram processados no Databricks, onde ocorreram as seguintes etapas:

1. Leitura dos dados armazenados na camada Silver:

```python
df_zone = spark.read.format('parquet')\
                .option('inferSchema', True)\
                .option('header', True)\
                .load('<caminho_da_camada_silver>')
```

2. Transforma√ß√£o e limpeza dos dados usando PySpark:
   - Convers√£o de tipos de dados.
   - Remo√ß√£o de valores nulos e duplicados.
   - Padroniza√ß√£o de colunas e normaliza√ß√£o de dados.

3. Salvamento dos dados processados em formato Delta na camada Gold:

```python
df_zone.write.format('delta')\
    .mode('append')\
    .option('path', '<caminho_da_camada_gold>')\
    .save()
```

4. Cria√ß√£o de tabelas otimizadas para an√°lises futuras, com particionamento e indexa√ß√£o dos dados.

### üìÅ **3. Gerenciamento de Acesso com Unity Catalog**

Para garantir o acesso seguro aos dados:

1. Configura√ß√£o de uma **Storage Credential**.
2. Cria√ß√£o de uma **External Location** para armazenar os dados no Data Lake.
3. Registro das tabelas no metastore do Databricks para facilitar a an√°lise:

```sql
CREATE TABLE IF NOT EXISTS gold.trip_zone
USING DELTA
LOCATION '<caminho_da_camada_gold>';
```

4. Defini√ß√£o de permiss√µes de leitura e escrita por usu√°rio e grupo, assegurando o controle de acesso granular.

---

## üìÇ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ data_extraction/        # Configura√ß√µes e pipelines do ADF
‚îú‚îÄ‚îÄ data_transformation/    # Notebooks de transforma√ß√£o de dados no Databricks
‚îú‚îÄ‚îÄ scripts/                # Scripts de automa√ß√£o e suporte
‚îú‚îÄ‚îÄ notebooks/              # Exporta√ß√£o dos notebooks do Databricks
‚îú‚îÄ‚îÄ tests/                  # Scripts de testes de integridade dos dados
‚îú‚îÄ‚îÄ documentation/          # Documenta√ß√£o t√©cnica do projeto
‚îú‚îÄ‚îÄ README.md               # Documenta√ß√£o do projeto
‚îú‚îÄ‚îÄ .gitignore              # Arquivos a serem ignorados no controle de vers√£o
```

---

## ‚úÖ Principais Funcionalidades

- Extra√ß√£o automatizada de dados din√¢micos diretamente de uma URL p√∫blica usando ADF.
- Transforma√ß√£o e valida√ß√£o de dados com PySpark no Databricks.
- Armazenamento otimizado em Delta Lake para an√°lises r√°pidas e eficientes.
- Controle de acesso seguro utilizando Unity Catalog e locais externos configurados.
- Implementa√ß√£o de testes de integridade para garantir a qualidade dos dados processados.
- Automatiza√ß√£o de fluxos de trabalho, com agendamentos peri√≥dicos e notifica√ß√µes de status.

---

## üìä Resultados Esperados

- Tabelas Delta registradas no metastore do Databricks, prontas para consultas anal√≠ticas avan√ßadas.
- Dados organizados e dispon√≠veis no Azure Data Lake Storage (Camada Gold), prontos para an√°lises e relat√≥rios.
- Relat√≥rios automatizados gerados diretamente a partir da camada Gold.
- Otimiza√ß√£o do tempo de processamento e redu√ß√£o de custos operacionais por meio de particionamento e caching eficiente.

---

